{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "from sklearn.datasets import fetch_mldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "mnist_data = fetch_mldata('MNIST original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DESCR': 'mldata.org dataset: mnist-original',\n",
       " 'COL_NAMES': ['label', 'data'],\n",
       " 'target': array([0., 0., 0., ..., 9., 9., 9.]),\n",
       " 'data': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check loaded dataset\n",
    "mnist_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets loaded using Scikit-Learn usually have the structure in which they have:\n",
    "\n",
    "1. \"DESCR\" key describing the dataset\n",
    "2. \"COL_NAMES\" key describing the name of the available columns\n",
    "3. \"target\" key that is the labels for our dataset\n",
    "4. \"data\" key that is the feature points for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Features and Targets\n",
    "\n",
    "# Features\n",
    "X = mnist_data['data']\n",
    "\n",
    "# Labels/Target\n",
    "y = mnist_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of Features\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of Labels\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that there are 70,000 images in the dataset and each image has 784 features as each image is 28 x 28 pixels and each feature represents one pixel's intensity.\n",
    "\n",
    "Let's plot a sample feature image. For that we'll have to reshape it into a 28x28 array and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a random digit from dataset features\n",
    "n = np.random.randint(0,X.shape[0],1)\n",
    "\n",
    "# Get random image data\n",
    "random_img = X[n]\n",
    "\n",
    "# Reshape data into 28x28 array to be shown as image\n",
    "random_img = random_img.reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of random_img\n",
    "random_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the random image\n",
    "plt.imshow(random_img, cmap='gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the corresponding label for this number\n",
    "random_img_label = y[n]\n",
    "\n",
    "random_img_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before inspecting the data further, we should create a train and test dataset so that we do not see the test data till the very end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (60000,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the split data\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 784), (10000,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, let's shuffle the data so that the cross-validation folds will be similar in the future\n",
    "shuffle_idx = np.random.permutation(60000)\n",
    "X_train, y_train = X_train[shuffle_idx], y_train[shuffle_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Binary Classifier\n",
    "\n",
    "Now, that we have shuffled the training data and the labels, let's train a simple Binary Classifier to detect a \"9\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Classifier\n",
    "# True: for all 9's and False: for all the rest\n",
    "y_train_9 = (y_train == 9)\n",
    "y_test_9 = (y_test == 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_9.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_9.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hare krishna\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=101, shuffle=True,\n",
       "       tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import SGD CLassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Train a Stochastic Gradient Classifier\n",
    "# SGDClassifier relies on randomness during training, hence the name 'stochastic'\n",
    "clf = SGDClassifier(random_state = 101)\n",
    "clf.fit(X_train, y_train_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now use this classifier to see if it can detect a 9\n",
    "\n",
    "# Get a random Number\n",
    "n = np.random.randint(0,X_test.shape[0],1)\n",
    "\n",
    "# Make a prediction for that digit that if that is a 9 or not\n",
    "clf.predict(X_test[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So, what was the original test label for that number\n",
    "y_test[n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Measures\n",
    "\n",
    "## Measuring Accuracy using Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "# StratifiedKFold: Performs stratified sampling to produce folds that contain a representative ratio of each class\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize StraitifiedKFold\n",
    "# n_splits: Number of Folds\n",
    "skfolds = StratifiedKFold(n_splits=3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Shape:\n",
      "(40000, 784) (40000,)\n",
      "(20000, 784) (20000,)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hare krishna\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.93905\n",
      "\n",
      "Data Shape:\n",
      "(40000, 784) (40000,)\n",
      "(20000, 784) (20000,)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hare krishna\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.94955\n",
      "\n",
      "Data Shape:\n",
      "(40000, 784) (40000,)\n",
      "(20000, 784) (20000,)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hare krishna\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9115\n"
     ]
    }
   ],
   "source": [
    "# At each iteration, create a Clone of the classifier, train the Clone on training folds and make predicitons on test fold.\n",
    "for train_idx, test_idx in skfolds.split(X_train, y_train_9):\n",
    "    # Create a clone of the classifier\n",
    "    clf_clone = clone(clf)\n",
    "    # Create X_train folds\n",
    "    X_train_folds = X_train[train_idx]\n",
    "    # Create y_train folds\n",
    "    y_train_folds = y_train_9[train_idx]\n",
    "    # Create X_test_fold : Validation Set\n",
    "    X_test_fold = X_train[test_idx]\n",
    "    # Create y_test_fold : Validation Set\n",
    "    y_test_fold = y_train_9[test_idx]\n",
    "    \n",
    "    print('\\nData Shape:')\n",
    "    print(X_train_folds.shape, y_train_folds.shape)\n",
    "    print(X_test_fold.shape, y_test_fold.shape)\n",
    "    print('\\n')\n",
    "    \n",
    "    # Train the Cloned Classifier on Training Data and Evaluate it's performance on Validation Data\n",
    "    clf_clone.fit(X_train_folds,y_train_folds)\n",
    "    # Predicted Labels\n",
    "    y_pred = clf_clone.predict(X_test_fold)\n",
    "    # Number of Correct Predictions\n",
    "    num_correct = sum(y_pred == y_test_fold)\n",
    "    # Print Accuracy for 3 Cross-Validation Splits\n",
    "    print('Accuracy: ',num_correct/len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hare krishna\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "c:\\users\\hare krishna\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "c:\\users\\hare krishna\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.93905, 0.94955, 0.9115 ])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Scikit-Learn's Cross_val_score function to verify the above functionality\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Get Cross_val_score on trainig data with 3 splits\n",
    "cross_val_score(clf, X_train, y_train_9, cv=3, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Cross-Validation, we got an accuracy score of about 95% but since ***Accuracy is a High Variance Metric***, we should confirm this accuracy using other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple classifier to test out the accuracy for not a '9'\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "# Class to predict that the number is not a 9\n",
    "class not_a_nine(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self,X):\n",
    "        return np.zeros((len(X),1), dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90155, 0.89685, 0.90415])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiailize the Classifier and get the Cross_val_Score\n",
    "not_9_clf = not_a_nine()\n",
    "cross_val_score(not_9_clf, X_train, y_train_9, cv=3, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazingly, it shows an accuracy of 90%. This is because only 10% of the images are of digit '9'. So, if we guess tha an image is not a '9', we'll be right 90% of the time which is not good at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A much better way to find the performance of the model is to use the confusion matrix instead of the accuracy. The idea is to look at the False Positives and False Negatives and count that how many times the classifier missed to classify correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hare krishna\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "c:\\users\\hare krishna\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "c:\\users\\hare krishna\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import Dependencies\n",
    "# Cross_val_predict: performs K-fold cross-validation and returns the predictions made on each test fold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_pred = cross_val_predict(clf, X_train, y_train_9, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[51106,  2945],\n",
       "       [ 1053,  4896]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the Confusion Matrix for the Predictions\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_train_9, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above confusion matrix tells that the classifier was able to correctly classify the digit as a '9' 51,106 times. For, 2,945 times, it mistakenly classified some other number as a '9'. For 1053 times, it wrongly classified as not a '9' whereas 4,896 times it correctly classified not a '9'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall\n",
    "\n",
    "The confusion matrix provides us with a lot of information but sometimes we need more metrics to see the performance of our classifier.\n",
    "\n",
    "There are three such metrics that we can use:\n",
    "\n",
    "1. **Precision**:  True Positives / (True Positives + False Positives)\n",
    "\n",
    "2. **Recall/Sensitivity**:  True Positives / (True Positives + False Negatives)\n",
    "\n",
    "3. **F1-Score**: True Positives / (True Positive +  (False Negatives + False Positives) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
